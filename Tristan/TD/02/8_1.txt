(A):
In my case for this test I got an accuracy of 100% for both classes. 
Reason 1: class 0 is the opposite of class 1 due the the mean and same corresponding variance, meaning its easier to identify which is which.
Reason 2: the variance is rather small i.e. its noise impact will not be large, meaning classification becomes easier thus more accurate.
Reason 3: If we think about the distributions are also very far apart further indicating the unlikely event of a datapoint being between both distributions.

(B)
1. Increasing the number of datapoints n will lead to more accuracy and a more realistic comparison.
2. Setting the mean to unique values per class i.e. having it vastly different will cause the classification to be easier. (Very different mean values for each class).
3. Lowering the standard deviation results in less noice thus easier classification thus more accuracy.

Sidenote: Now if we image we a have a dataset of very similar classes each representing a distribution, the classification becomes harder thus more interesting.
If we dwelve even deeper into linear algebra we might also suggest that with increasing dimensions on any given data classification becomes harder and harder
